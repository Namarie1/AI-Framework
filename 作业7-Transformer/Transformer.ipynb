{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f5d4a7-e994-4365-9903-315598f9ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自注意力机制后的输出:\n",
      "tensor([[ 1.3634, -0.1174, -0.2121],\n",
      "        [ 1.4749, -0.1411, -0.2295],\n",
      "        [ 1.5752, -0.1625, -0.2451]])\n"
     ]
    }
   ],
   "source": [
    "# 计算注意力分数\n",
    "# 导入库\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 示例输入序列\n",
    "input_sequence = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "\n",
    "# 生成 Key、Query 和 Value 矩阵的随机权重\n",
    "random_weights_key = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_query = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_value = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "\n",
    "# 计算 Key、Query 和 Value 矩阵\n",
    "key = torch.matmul(input_sequence, random_weights_key)\n",
    "query = torch.matmul(input_sequence, random_weights_query)\n",
    "value = torch.matmul(input_sequence, random_weights_value)\n",
    "\n",
    "# 计算注意力分数\n",
    "attention_scores = torch.matmul(query, key.T) / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32))\n",
    "\n",
    "# 使用 softmax 函数获得注意力权重\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# 计算 Value 向量的加权和\n",
    "output = torch.matmul(attention_weights, value)\n",
    "\n",
    "print(\"自注意力机制后的输出:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bcf1c5-a2d3-42ff-8dd7-0ef89617e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入序列的位置编码:\n",
      "torch.Size([5, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# 位置编码的实现\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        # 继承nn.Module的初始化方法\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # 计算位置编码\n",
    "        # 初始化一个(max_len, d_model)的零张量用于存储位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 创建一个从0到max_len-1的张量，用于表示每个位置的索引，并在第1维增加一个尺寸\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算缩放因子，用于调整正弦波和余弦波的频率\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        # 根据位置编码公式填充偶数列（正弦部分）\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 填充奇数列（余弦部分）\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为位置编码增加一个批次维度，以便于后续与输入数据相加\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 使用register_buffer将位置编码缓存为模型的一部分，不会被梯度更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在前向传播中，将位置编码直接加到输入x上，注意重复或裁剪以匹配x的序列长度\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# 示例用法\n",
    "d_model = 512  # 嵌入维度\n",
    "max_len = 100  # 序列最大长度\n",
    "\n",
    "# 位置编码实例化\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# 创建一个示例输入序列，形状为(批量大小, 序列长度, 嵌入维度)\n",
    "input_sequence = torch.randn(5, max_len, d_model)\n",
    "\n",
    "# 应用位置编码到输入序列上\n",
    "input_sequence = pos_encoder(input_sequence)\n",
    "\n",
    "print(\"输入序列的位置编码:\")\n",
    "print(input_sequence.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3119556c-8d3f-4887-ba23-2150635cf1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_output shape: torch.Size([5, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# 多头注意力的代码实现\n",
    "\n",
    "# 定义一个多头注意力（Multi-Head Attention）类，继承自nn.Module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        # 调用父类初始化方法\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # 设置头的数量\n",
    "        self.num_heads = num_heads\n",
    "        # 设置模型的嵌入维度\n",
    "        self.d_model = d_model\n",
    "        # 断言检查d_model是否能被num_heads整除，确保可以平均分配给每个头\n",
    "        assert d_model % num_heads == 0\n",
    "        # 计算每个头的维度\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        # 定义线性变换层，用于查询、键、值的转换\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 定义一个输出的线性变换层，用于最终的注意力输出\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # 接收一个张量并将其按头分割\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        # 调整张量形状以便于分头，然后转置以适应多头注意力的计算\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.depth).transpose(1, 2)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 对查询、键、值应用线性变换\n",
    "        query = self.query_linear(query)\n",
    "        key = self.key_linear(key)\n",
    "        value = self.value_linear(value)\n",
    "        \n",
    "        # 将变换后的查询、键、值张量按头部进行分割\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        \n",
    "        # 计算缩放点积注意力分数，除以深度的平方根是为了缩放，防止softmax函数因太大或太小的值而饱和\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.depth)\n",
    "        \n",
    "        # 如果提供了掩码，用负无穷大填充那些需要被屏蔽的位置，以便在softmax中忽略这些位置\n",
    "        if mask is not None:\n",
    "            scores += scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 应用softmax函数得到注意力权重\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 使用注意力权重加权求和值得到注意力输出\n",
    "        attention_output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        # 将多头注意力的结果合并回原始形状\n",
    "        batch_size, _, seq_length, d_k = attention_output.size()\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "        # 对最终的注意力输出做线性变换\n",
    "        attention_output = self.output_linear(attention_output)\n",
    "        \n",
    "        # 返回注意力输出结果\n",
    "        return attention_output\n",
    "\n",
    "# 示例代码：使用定义的多头注意力类\n",
    "# 设置模型的嵌入维度、序列最大长度、头的数量以及潜在的前馈网络维度（尽管这里未直接使用）\n",
    "d_model = 512\n",
    "max_len = 100\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "\n",
    "# 创建一个多头注意力实例\n",
    "multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 随机生成一个示例输入序列张量\n",
    "input_sequence = torch.randn(5, max_len, d_model)\n",
    "\n",
    "# 应用多头注意力于输入序列自身（查询=键=值）\n",
    "attention_output= multihead_attn(input_sequence, input_sequence, input_sequence)\n",
    "\n",
    "# 打印输出注意力结果的形状\n",
    "print(\"attention_output shape:\", attention_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0949562f-6851-4f1e-99dd-ffd0c636c214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequence shape: torch.Size([5, 100, 512])\n",
      "output_ff shape: torch.Size([5, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# 前馈网络的代码实现\n",
    "# 定义一个前馈网络（FeedForward Network）模块，作为Transformer模型的一部分，用于在多头注意力之后增加网络的非线性表达能力。\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        # 继承nn.Module的初始化方法，并定义前馈网络的结构\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 第一层线性变换，将输入维度d_model映射到一个更大的维度d_ff，有助于模型学习更复杂的表示\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        # 第二层线性变换，将中间维度d_ff映射回原始的输入维度d_model，保持输出与输入维度一致\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        # 使用ReLU激活函数，为网络引入非线性\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 线性变换1后接ReLU激活，增加网络的非线性表达能力\n",
    "        x = self.relu(self.linear1(x))\n",
    "        \n",
    "        # 第二次线性变换，将激活后的特征映射回d_model维度\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # 返回前馈网络的输出\n",
    "        return x\n",
    "\n",
    "# 定义模型使用的超参数\n",
    "d_model = 512         # 模型的嵌入维度\n",
    "max_len = 100          # 序列的最大长度\n",
    "num_heads = 8          # 多头注意力中头的数量\n",
    "d_ff = 2048           # 前馈网络中间层的维度\n",
    "\n",
    "# 实例化多头注意力模块\n",
    "multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 实例化前馈网络模块\n",
    "ff_network = FeedForward(d_model, d_ff)\n",
    "\n",
    "# 生成一个随机的输入序列，用于演示\n",
    "input_sequence = torch.randn(5, max_len, d_model) # 形状为(批次大小, 序列长度, 嵌入维度)\n",
    "\n",
    "# 应用多头注意力机制到输入序列上\n",
    "attention_output = multihead_attn(input_sequence, input_sequence, input_sequence)\n",
    "\n",
    "# 将多头注意力的输出传入前馈网络\n",
    "output_ff = ff_network(attention_output)\n",
    "\n",
    "# 打印输入序列和前馈网络输出的形状，验证数据流正确无误\n",
    "print('input_sequence shape:', input_sequence.shape)\n",
    "print(\"output_ff shape:\", output_ff.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ace85d-ad9e-4f69-ba5d-44a4f0c86150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape： torch.Size([5, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# 编码器的代码实现\n",
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义多头注意力模块（Multi-Head Attention）和前馈网络模块（FeedForward），\n",
    "# 这里假设它们已被正确定义在代码的其他部分，因为它们的具体实现未给出。\n",
    "# 注意：实际使用中，你需要实现这些类。\n",
    "\n",
    "# 位置编码类，为输入序列中的每个位置生成一个固定的向量表示\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 初始化位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 将位置编码加到输入序列上\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# 编码器层，结合自注意力机制和前馈网络，同时包含Layer Normalization和Dropout\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # 初始化自注意力层、前馈网络层、归一化层以及Dropout层\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)  # 多头注意力层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)              # 前馈网络层\n",
    "        self.norm1 = nn.LayerNorm(d_model)                          # 第一层归一化\n",
    "        self.norm2 = nn.LayerNorm(d_model)                          # 第二层归一化\n",
    "        self.dropout = nn.Dropout(dropout)                          # Dropout层\n",
    "        self.positional_encoding = PositionalEncoding(d_model)       # 位置编码\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # 应用位置编码\n",
    "        x = self.positional_encoding(x)\n",
    "        # 自注意力层处理并添加残差连接\n",
    "        attention_output = self.self_attention(x, x, x, mask)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        x = x + attention_output\n",
    "        x = self.norm1(x)\n",
    "        # 前馈网络处理并添加残差连接\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        feed_forward_output = self.dropout(feed_forward_output)\n",
    "        x = x + feed_forward_output\n",
    "        x = self.norm2(x)\n",
    "        return x \n",
    "\n",
    "# 设置模型超参数\n",
    "d_model = 512         # 模型维度\n",
    "max_len = 100         # 序列最大长度\n",
    "num_heads = 8         # 多头注意力中的头数\n",
    "d_ff = 2048           # 前馈网络的隐藏层尺寸\n",
    "\n",
    "# 实例化编码器层和位置编码模块\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff, 0.1)\n",
    "pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "# 创建一个随机的输入序列张量，模拟数据输入\n",
    "input_sequence = torch.randn(5, max_len, d_model)  # 形状为(批量大小, 序列长度, 模型维度)\n",
    "\n",
    "# 使用位置编码增强输入序列\n",
    "input_with_pe = pos_encoder(input_sequence)\n",
    "\n",
    "# 通过编码器层处理带位置编码的输入序列，此处未使用遮罩（mask）\n",
    "encoder_output = encoder_layer(input_with_pe, None)\n",
    "\n",
    "# 打印输出的形状\n",
    "print(\"encoder output shape：\", encoder_output.shape)  # 预期输出为 torch.Size([5, 100, 512])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e2cf48-2b2d-4681-9189-71d1e4cdbd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# 解码器的代码实现\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        初始化解码器层所需的所有子层和归一化层。\n",
    "        \n",
    "        :param d_model: 模型的维度\n",
    "        :param num_heads: 多头注意力中的头数\n",
    "        :param d_ff: 前馈网络中间层的维度\n",
    "        :param dropout: Dropout比率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)  # 掩码自注意力层\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model, num_heads)      # 编码器-解码器注意力层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)                      # 前馈网络层\n",
    "        self.norm1 = nn.LayerNorm(d_model)                                  # 第一层归一化\n",
    "        self.norm2 = nn.LayerNorm(d_model)                                  # 第二层归一化\n",
    "        self.norm3 = nn.LayerNorm(d_model)                                  # 第三层归一化\n",
    "        self.dropout = nn.Dropout(dropout)                                  # Dropout层\n",
    "\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        解码器层的前向传播方法，执行自注意力、编码器-解码器注意力和前馈网络操作。\n",
    "        \n",
    "        :param x: 解码器的输入\n",
    "        :param encoder_output: 编码器的输出\n",
    "        :param src_mask: 源序列的遮罩\n",
    "        :param tgt_mask: 目标序列的遮罩\n",
    "        :return: 经过解码器层处理后的输出\n",
    "        \"\"\"       \n",
    "        \n",
    "        # 掩码自注意力层\n",
    "        self_attention_output = self.masked_self_attention(x, x, x, tgt_mask)\n",
    "        self_attention_output = self.dropout(self_attention_output)\n",
    "        x = x + self_attention_output \n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # 编码器-解码器注意力层\n",
    "        enc_dec_attention_output = self.enc_dec_attention(x, encoder_output,\n",
    "                                                          encoder_output, src_mask)\n",
    "        enc_dec_attention_output = self.dropout(enc_dec_attention_output)\n",
    "        x = x + enc_dec_attention_output\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # 前馈层\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        feed_forward_output = self.dropout(feed_forward_output)\n",
    "        x = x + feed_forward_output\n",
    "        x = self.norm3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "# 定义DecoderLayer的参数\n",
    "max_len = 100\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "batch_size = 1\n",
    "    \n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff, 0.1)\n",
    "pos_encoder = PositionalEncoding(d_model)\n",
    "# 创建一个随机的输入序列张量，模拟数据输入\n",
    "input_sequence = torch.randn(5, max_len, d_model)  # 形状为(批量大小, 序列长度, 模型维度)\n",
    "# 使用位置编码增强输入序列\n",
    "input_with_pe = pos_encoder(input_sequence)\n",
    "# 通过编码器层处理带位置编码的输入序列，此处未使用遮罩（mask）\n",
    "encoder_output = encoder_layer(input_with_pe, None)\n",
    "\n",
    "\n",
    "# 定义DecoderLayer实例\n",
    "decoder_layer = DecoderLayer(d_model, num_heads, d_ff, dropout)      \n",
    "\n",
    "''' \n",
    "1. torch.rand(batch_size, max_len, max_len) 生成一个形状为(batch_size, max_len, max_len)的张量，\n",
    "其中每个元素都是从0到1之间的随机数。\n",
    "2. > 0.5 是一个布尔操作，将张量中的每个元素与0.5比较，如果元素大于0.5，\n",
    "则结果为True（通常在PyTorch中表示为1），否则为False（通常表示为0）。\n",
    "因此，这行代码实际上生成了一个二值掩码，其中大部分情况下会是随机分布的True和False\n",
    "，但实际上在Transformer的上下文中，src_mask常用于表示源序列中的padding部分或者\n",
    "在某些情境下对序列进行特殊处理，此处的生成方式可能不直接对应标准实践，\n",
    "标准情况下src_mask应依据实际的padding情况精确构建。\n",
    "'''\n",
    "src_mask = torch.rand(batch_size, max_len, max_len) > 0.5\n",
    "''' \n",
    "1. torch.ones(max_len, max_len) 创建一个形状为(max_len, max_len)的全1张量，\n",
    "这里的max_len和max_len应当理解为同一概念，表示序列长度。\n",
    "2. torch.tril(...) 是一个三角函数，它返回下三角矩阵的下三角部分，其中下三角部分的所有元素为1，\n",
    "其余为0。在这个上下文中，它生成了一个下三角掩码，\n",
    "用于确保自注意力（在解码器的自注意力层）中只关注当前位置及之前的位置，符合自回归原则。\n",
    "3. unsqueeze(0) 在张量的第一个维度（批维度）添加一个维度，使得掩码可以广播到任何批次大小，\n",
    "最终形状变为(1, max_len, max_len)。\n",
    "4. == 0 这个操作将下三角矩阵转换为掩码形式，其中下三角为False（0，表示可访问），\n",
    "上三角为True（1，经过后续处理后变为0，表示不可访问），以确保自注意力不会看到未来的信息。\n",
    "'''\n",
    "tgt_mask = torch.tril(torch.ones(max_len, max_len)).unsqueeze(0) == 0\n",
    "\n",
    "#将输入张量传递到DecoderLayer\n",
    "output = decoder_layer(input_sequence, encoder_output, src_mask, tgt_mask)  \n",
    "\n",
    "print(\"Output shape:\", output.shape)  # torch.Size([5, 100, 512])    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5359376b-cb86-468f-b799-19f7b2a9723e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 99, 5000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer的实现\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers,\n",
    "                 d_ff, max_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # 定义编码器和解码器的词嵌入层\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # 定义位置编码层\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # 定义编码器和解码器的多层堆叠\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # 定义线性层\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # 生成掩码\n",
    "    def generate_mask(self, src, tgt):\n",
    "        \n",
    "        ''' \n",
    "        通过比较src（源序列）与0，创建一个布尔掩码，其中1表示有效 token（非填充值，假设0为填充标记），\n",
    "        0表示填充部分。然后，使用.unsqueeze(1)和.unsqueeze(2)分别在第1维和第2维增加一维，\n",
    "        以便该掩码能够正确广播并与后续操作中的注意力权重矩阵相乘。\n",
    "        这主要用于排除填充部分在自注意力计算中的影响。\n",
    "        '''\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        seq_length = tgt.size(1)\n",
    "        \n",
    "        ''' \n",
    "        这里计算了一个“nopeek”掩码，用于阻止解码器中的一个时间步访问之后的时间步，\n",
    "        这是Transformer解码器中的典型做法，以维持自回归特性。\n",
    "        它通过创建一个上三角矩阵（torch.triu）并取其补（减去自身并取1减去结果）来实现，\n",
    "        然后转换为布尔类型。seq_length是从目标序列tgt中获取的实际序列长度。\n",
    "        '''\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        ''' \n",
    "        将之前初始化的目标序列掩码（排除了padding）与刚计算的“nopeek”掩码进行按位与操作（&），\n",
    "        这样得到的tgt_mask既排除了padding，\n",
    "        又阻止了每个位置看到未来的信息，符合Transformer解码器的需要。\n",
    "        '''\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        \n",
    "        ''' \n",
    "        ，函数返回两个掩码：一个是源序列掩码，用于在编码器中屏蔽padding；\n",
    "        另一个是经过调整的目标序列掩码，用于在解码器中同时屏蔽padding和未来信息。\n",
    "        '''\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    # 前向传播\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        # 编码器输入的词嵌入和位置编码\n",
    "        encoder_embedding = self.encoder_embedding(src)\n",
    "        en_positional_encoding = self.positional_encoding(encoder_embedding)\n",
    "        src_embedded = self.dropout(en_positional_encoding)\n",
    "        \n",
    "        # 解码器输入的词嵌入和位置编码\n",
    "        decoder_embedding = self.decoder_embedding(tgt)\n",
    "        de_positional_encoding = self.positional_encoding(decoder_embedding)\n",
    "        tgt_embedded = self.dropout(de_positional_encoding)\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "            \n",
    "        output = self.linear(dec_output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "# 示例用法\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048 \n",
    "max_len = 100\n",
    "dropout = 0.1 \n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, \n",
    "                          d_ff, max_len, dropout)\n",
    "\n",
    "# 生成随机示例数据\n",
    "''' \n",
    "这行代码的作用是生成一个形状为(5, max_len)的张量，其中的每个元素都是在1到src_vocab_size之间（不包括src_vocab_size）\n",
    "的随机整数。这个张量可以用来模拟一个包含5个样本、每个样本长度为max_len的源序列数据，\n",
    "适用于训练如机器翻译、文本生成等序列到序列（sequence-to-sequence）的学习任务中。\n",
    "每个元素可以被视作词汇表中一个单词的索引，用于后续的词嵌入等处理。\n",
    "'''\n",
    "src_data = torch.randint(1, src_vocab_size, (5, max_len))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (5, max_len))  # (batch_size, seq_length)\n",
    "\n",
    "transformer(src_data, tgt_data[:, :-1]).shape  # torch.Size([5, 99, 5000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05497fc2-8525-4645-ab0b-c10674523bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 轮：损失= 8.7046\n",
      "2 轮：损失= 8.2527\n",
      "3 轮：损失= 8.0162\n",
      "4 轮：损失= 7.8507\n",
      "5 轮：损失= 7.6828\n",
      "6 轮：损失= 7.5305\n",
      "7 轮：损失= 7.3640\n",
      "8 轮：损失= 7.2285\n",
      "9 轮：损失= 7.0601\n",
      "10 轮：损失= 6.8446\n",
      "\n",
      "虚拟数据的评估损失= 8.7007\n"
     ]
    }
   ],
   "source": [
    "# Transformer 模型的训练和评估流程实现\n",
    "\n",
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化损失函数 CrossEntropyLoss，设置忽略索引为0，通常用于padding部分的mask处理\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# 初始化优化器 Adam，用于更新Transformer模型参数\n",
    "# 学习率为0.0001，betas=(0.9, 0.98)为动量项系数，eps为防止除零异常的最小分母值\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 开始训练循环\n",
    "transformer.train()  # 设置模型为训练模式\n",
    "\n",
    "# 进行多个epoch的训练\n",
    "for epoch in range(10):  # 假设训练10轮\n",
    "    # 在每个批次开始前清零梯度，避免梯度累积\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 通过Transformer模型得到输出，src_data为源序列，tgt_data[:, :-1]为目标序列去掉最后一个词作为输入\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    \n",
    "    # 计算损失，输出和目标序列都调整为一维向量形式以匹配CrossEntropyLoss要求\n",
    "    ''' \n",
    "    output.contiguous().view(-1, tgt_vocab_size):\n",
    "        .contiguous()确保张量的内存是连续的。在某些操作后（如transpose），张量可能在内存中变得非连续，\n",
    "        这会影响之后的.view()操作。虽然在现代PyTorch版本中这一步有时不是必需的，但在涉及改变张量形状时\n",
    "        保持习惯性地使用可以避免潜在问题。\n",
    "        .view(-1, tgt_vocab_size) 改变output的形状。-1是一个占位符，表示该维度的大小由其他维度自动推断得出，\n",
    "        以保证整体元素数量不变。tgt_vocab_size是你目标词汇表的大小。\n",
    "        这意味着将输出张量重塑为形状(batch_size * sequence_length, tgt_vocab_size)，\n",
    "        其中每一行对应一个单词的预测概率分布。\n",
    "    '''\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, :-1].contiguous().view(-1))\n",
    "    \n",
    "    # 反向传播计算梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新模型参数\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 打印当前epoch的损失信息\n",
    "    print(f\"{epoch + 1} 轮：损失= {loss.item():.4f}\")\n",
    "\n",
    "# 准备虚拟数据用于演示，模拟实际的输入输出数据\n",
    "src_data= torch.randint(1, src_vocab_size, (5, max_len))  # 源序列数据\n",
    "tgt_data= torch.randint(1, tgt_vocab_size, (5, max_len))  # 目标序列数据\n",
    "\n",
    "# 开始评估循环，设置模型为评估模式以关闭dropout等训练时特有的操作\n",
    "transformer.eval()\n",
    "\n",
    "# 使用torch.no_grad()上下文管理器，避免在此阶段计算和存储梯度，节省内存并加速评估过程\n",
    "with torch.no_grad():\n",
    "    # 对虚拟数据进行前向传播得到预测输出\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    \n",
    "    # 计算评估阶段的损失，注意这里tgt_data切片变化，与训练阶段对应未来词预测\n",
    "    ''' \n",
    "    切片操作[:, 1:]意味着取所有行（样本），但是从每个序列的第二个元素到最后一个元素，这是因为序列到序列任务中，\n",
    "    模型在训练时通常需要预测下一个词，所以标签序列相比输入序列少一个元素（第一个词不参与预测）。\n",
    "    '''\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    \n",
    "    # 输出评估损失\n",
    "    print(f\"\\n虚拟数据的评估损失= {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abb871f-af99-46bc-93f0-a33cd4cc6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.3562\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# 定义位置编码类\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# 定义多头注意力类\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // num_heads\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.depth).transpose(1, 2)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.query_linear(query)\n",
    "        key = self.key_linear(key)\n",
    "        value = self.value_linear(value)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.depth)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, value)\n",
    "        batch_size, _, seq_length, d_k = attention_output.size()\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        attention_output = self.output_linear(attention_output)\n",
    "        return attention_output\n",
    "\n",
    "# 定义前馈网络类\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# 定义编码器层类\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.positional_encoding(x)\n",
    "        attention_output = self.self_attention(x, x, x, mask)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        x = x + attention_output\n",
    "        x = self.norm1(x)\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        feed_forward_output = self.dropout(feed_forward_output)\n",
    "        x = x + feed_forward_output\n",
    "        x = self.norm2(x)\n",
    "        return x \n",
    "\n",
    "# 定义解码器层类\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        self_attention_output = self.masked_self_attention(x, x, x, tgt_mask)\n",
    "        self_attention_output = self.dropout(self_attention_output)\n",
    "        x = x + self_attention_output \n",
    "        x = self.norm1(x)\n",
    "        enc_dec_attention_output = self.enc_dec_attention(x, encoder_output,\n",
    "                                                          encoder_output, src_mask)\n",
    "        enc_dec_attention_output = self.dropout(enc_dec_attention_output)\n",
    "        x = x + enc_dec_attention_output\n",
    "        x = self.norm2(x)\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        feed_forward_output = self.dropout(feed_forward_output)\n",
    "        x = x + feed_forward_output\n",
    "        x = self.norm3(x)\n",
    "        return x\n",
    "\n",
    "# 定义Transformer类\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers,\n",
    "                 d_ff, max_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.encoder_embedding(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "    \n",
    "    def decode(self, tgt, memory, src_mask, tgt_mask):\n",
    "        tgt = self.decoder_embedding(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
    "        return tgt\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        memory = self.encode(src, src_mask)\n",
    "        output = self.decode(tgt, memory, src_mask, tgt_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "# 定义模型参数\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_len = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# 创建Transformer模型实例\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers,\n",
    "                    d_ff, max_len, dropout)\n",
    "\n",
    "# 生成随机的输入数据\n",
    "src = torch.randint(0, src_vocab_size, (32, 10))  # 模拟源序列\n",
    "tgt = torch.randint(0, tgt_vocab_size, (32, 10))  # 模拟目标序列\n",
    "\n",
    "# 前向传播\n",
    "output = model(src, tgt)\n",
    "\n",
    "# 示例评估翻译质量的函数\n",
    "def evaluate_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    评估BLEU分数的辅助函数\n",
    "    :param reference: 参考翻译\n",
    "    :param candidate: 模型生成的翻译\n",
    "    :return: BLEU分数\n",
    "    \"\"\"\n",
    "    smooth = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference], candidate, smoothing_function=smooth)\n",
    "reference = [\"this\", \"is\", \"a\", \"test\"]\n",
    "candidate = [\"this\", \"is\", \"a\",\"a\",\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "# 计算BLEU分数\n",
    "bleu_score = evaluate_bleu(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798edaa9-2305-4d67-b3c8-14a186ca3fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a29ecf-bca8-46e3-a4f7-a07e7a7b6c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbc8da-4771-49aa-80ba-3f66f45e948f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa885f4-ffc9-4538-b3f3-1a15f765383f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151c51d-c126-48b5-9575-afff2fefedd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6f0df-7369-4700-a706-05eb50e72b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73a271-24a6-463e-846b-f26300c159f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25fb899-cb9a-4010-9155-230e9182921c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00b865-045a-421f-a381-8c8ad2b7bf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
