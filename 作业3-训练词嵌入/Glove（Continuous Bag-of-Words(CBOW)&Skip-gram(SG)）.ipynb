{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29fd6d8",
   "metadata": {},
   "source": [
    "# Glove（Continuous Bag-of-Words(CBOW)&Skip-gram(SG)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88e1dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n",
      "EPOCH: 0 LOSS: 139.01980951986334\n",
      "EPOCH: 1 LOSS: 128.00263892032214\n",
      "EPOCH: 2 LOSS: 117.85713324639025\n",
      "EPOCH: 3 LOSS: 108.51319199052521\n",
      "EPOCH: 4 LOSS: 99.90666134580957\n",
      "EPOCH: 5 LOSS: 91.97876119545013\n",
      "EPOCH: 6 LOSS: 84.67558295118782\n",
      "EPOCH: 7 LOSS: 77.94764589861003\n",
      "EPOCH: 8 LOSS: 71.74950257517833\n",
      "EPOCH: 9 LOSS: 66.03938579239045\n",
      "EPOCH: 10 LOSS: 60.77889144799829\n",
      "EPOCH: 11 LOSS: 55.93269241691442\n",
      "EPOCH: 12 LOSS: 51.46827967112346\n",
      "EPOCH: 13 LOSS: 47.355727437217446\n",
      "EPOCH: 14 LOSS: 43.56747970995807\n",
      "EPOCH: 15 LOSS: 40.07815584058814\n",
      "EPOCH: 16 LOSS: 36.864373237484344\n",
      "EPOCH: 17 LOSS: 33.904585474393045\n",
      "EPOCH: 18 LOSS: 31.17893431264145\n",
      "EPOCH: 19 LOSS: 28.669114319145823\n",
      "EPOCH: 20 LOSS: 26.35824890970003\n",
      "EPOCH: 21 LOSS: 24.23077677282664\n",
      "EPOCH: 22 LOSS: 22.27234773783123\n",
      "EPOCH: 23 LOSS: 20.46972724495155\n",
      "EPOCH: 24 LOSS: 18.810708658175418\n",
      "EPOCH: 25 LOSS: 17.284032734361276\n",
      "EPOCH: 26 LOSS: 15.879313627247583\n",
      "EPOCH: 27 LOSS: 14.586970862967055\n",
      "EPOCH: 28 LOSS: 13.398166775735067\n",
      "EPOCH: 29 LOSS: 12.304748939219108\n",
      "EPOCH: 30 LOSS: 11.299197171341373\n",
      "EPOCH: 31 LOSS: 10.374574728437663\n",
      "EPOCH: 32 LOSS: 9.52448333922931\n",
      "EPOCH: 33 LOSS: 8.743021760335088\n",
      "EPOCH: 34 LOSS: 8.024747563381645\n",
      "EPOCH: 35 LOSS: 7.364641889449026\n",
      "EPOCH: 36 LOSS: 6.758076929865331\n",
      "EPOCH: 37 LOSS: 6.200785913467955\n",
      "EPOCH: 38 LOSS: 5.688835399582434\n",
      "EPOCH: 39 LOSS: 5.218599693318368\n",
      "EPOCH: 40 LOSS: 4.786737215512954\n",
      "EPOCH: 41 LOSS: 4.3901686739197805\n",
      "EPOCH: 42 LOSS: 4.026056895182265\n",
      "EPOCH: 43 LOSS: 3.6917881888751705\n",
      "EPOCH: 44 LOSS: 3.3849551255593635\n",
      "EPOCH: 45 LOSS: 3.103340620479311\n",
      "EPOCH: 46 LOSS: 2.8449032233362033\n",
      "EPOCH: 47 LOSS: 2.6077635225776086\n",
      "EPOCH: 48 LOSS: 2.390191579936608\n",
      "EPOCH: 49 LOSS: 2.1905953175996125\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# 超参数\n",
    "setting = {\n",
    "    \"window\": 2,  # 窗口尺寸（至左边或右边，即左边2个和右边2个）\n",
    "    \"n\": 8,  # 词向量维度\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"x_max\": 100,\n",
    "    \"alpha\": 0.75\n",
    "}\n",
    "\n",
    "class GloVe():\n",
    "    def __init__(self):\n",
    "        self.n = setting['n']\n",
    "        self.epochs = setting['epochs']\n",
    "        self.learning_rate = setting['learning_rate']\n",
    "        self.x_max = setting['x_max']\n",
    "        self.alpha = setting['alpha']\n",
    "\n",
    "    def generate_train_data(self, setting, corpus_file):\n",
    "        \"\"\"建立训练用的词向量\"\"\"\n",
    "        # 计算非重复词语的频数和共现矩阵\n",
    "        word_counts = defaultdict(int)\n",
    "        co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                tokens = line.strip().split()\n",
    "                for i, target_word in enumerate(tokens):\n",
    "                    word_counts[target_word] += 1\n",
    "                    context_words = tokens[max(0, i - setting['window']):i] + tokens[i + 1:i + setting['window'] + 1]\n",
    "                    for context_word in context_words:\n",
    "                        co_occurrence_matrix[target_word][context_word] += 1\n",
    "                        co_occurrence_matrix[context_word][target_word] += 1\n",
    "\n",
    "        self.count_word = len(word_counts.keys())  # 非重复词语数量\n",
    "        self.words_list = list(word_counts.keys())  # 非重语料列表\n",
    "        self.word_idx = dict((word, i) for i, word in enumerate(self.words_list))  # 建立word_idx 字典，加快访问速度\n",
    "        self.idx_word = dict((i, word) for i, word in enumerate(self.words_list))  # 建立idx_word 字典，加快访问速度\n",
    "\n",
    "        # 构建共现矩阵和权重矩阵\n",
    "        X = np.zeros((self.count_word, self.count_word))\n",
    "        for target_word, context_dict in co_occurrence_matrix.items():\n",
    "            for context_word, count in context_dict.items():\n",
    "                i, j = self.word_idx[target_word], self.word_idx[context_word]\n",
    "                X[i, j] = count\n",
    "\n",
    "        return X\n",
    "\n",
    "    def train(self, X):\n",
    "        \"\"\"训练\"\"\"\n",
    "        # 初始化权重\n",
    "        self.W = np.random.uniform(-0.5 / self.n, 0.5 / self.n, (self.count_word, self.n))\n",
    "        self.b = np.random.uniform(-0.5 / self.n, 0.5 / self.n, self.count_word)\n",
    "        self.U = np.random.uniform(-0.5 / self.n, 0.5 / self.n, (self.count_word, self.n))\n",
    "        self.c = np.random.uniform(-0.5 / self.n, 0.5 / self.n, self.count_word)\n",
    "\n",
    "        # 计算权重\n",
    "        f_x = np.vectorize(lambda x: (x / self.x_max) ** self.alpha if x < self.x_max else 1)\n",
    "        X = f_x(X)\n",
    "\n",
    "        # 遍历每个epoch\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = 0\n",
    "            for i in range(self.count_word):\n",
    "                for j in range(self.count_word):\n",
    "                    if X[i, j] > 0:\n",
    "                        # 计算预测值\n",
    "                        diff = np.dot(self.W[i], self.U[j]) + self.b[i] + self.c[j] - np.log(X[i, j])\n",
    "                        # 计算梯度\n",
    "                        grad_W = 2 * diff * self.U[j]\n",
    "                        grad_U = 2 * diff * self.W[i]\n",
    "                        grad_b = 2 * diff\n",
    "                        grad_c = 2 * diff\n",
    "                        # 更新参数\n",
    "                        self.W[i] -= self.learning_rate * grad_W\n",
    "                        self.U[j] -= self.learning_rate * grad_U\n",
    "                        self.b[i] -= self.learning_rate * grad_b\n",
    "                        self.c[j] -= self.learning_rate * grad_c\n",
    "                        # 计算损失\n",
    "                        loss += diff ** 2\n",
    "\n",
    "            print('EPOCH:', epoch, 'LOSS:', loss)\n",
    "\n",
    "# 实例化模型\n",
    "glove = GloVe()\n",
    "X = glove.generate_train_data(setting, r'C:\\Users\\zhangxiaomi\\Desktop\\chengyu.txt')  # 中文小语料库\n",
    "print(X.shape)\n",
    "\n",
    "glove.train(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
