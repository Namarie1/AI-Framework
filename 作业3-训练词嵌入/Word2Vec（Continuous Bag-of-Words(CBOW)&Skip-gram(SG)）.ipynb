{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070aff79",
   "metadata": {},
   "source": [
    "# Word2Vec（Continuous Bag-of-Words(CBOW)&Skip-gram(SG)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f8fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2)\n",
      "EPOCH: 0 LOSS: 55.224879099445474\n",
      "EPOCH: 1 LOSS: 54.196646055363274\n",
      "EPOCH: 2 LOSS: 53.192748181489804\n",
      "EPOCH: 3 LOSS: 52.21212860245161\n",
      "EPOCH: 4 LOSS: 51.25379138206123\n",
      "EPOCH: 5 LOSS: 50.31679803317917\n",
      "EPOCH: 6 LOSS: 49.400264262857064\n",
      "EPOCH: 7 LOSS: 48.503356935242216\n",
      "EPOCH: 8 LOSS: 47.62529123543032\n",
      "EPOCH: 9 LOSS: 46.76532801827887\n",
      "EPOCH: 10 LOSS: 45.92277132709312\n",
      "EPOCH: 11 LOSS: 45.0969660680321\n",
      "EPOCH: 12 LOSS: 44.28729582703142\n",
      "EPOCH: 13 LOSS: 43.49318081697999\n",
      "EPOCH: 14 LOSS: 42.71407594380799\n",
      "EPOCH: 15 LOSS: 41.94946898103367\n",
      "EPOCH: 16 LOSS: 41.19887884317104\n",
      "EPOCH: 17 LOSS: 40.4618539492154\n",
      "EPOCH: 18 LOSS: 39.73797066819708\n",
      "EPOCH: 19 LOSS: 39.02683183952553\n",
      "EPOCH: 20 LOSS: 38.328065361535415\n",
      "EPOCH: 21 LOSS: 37.641322842294244\n",
      "EPOCH: 22 LOSS: 36.96627830733808\n",
      "EPOCH: 23 LOSS: 36.30262695957007\n",
      "EPOCH: 24 LOSS: 35.650083987085\n",
      "EPOCH: 25 LOSS: 35.00838341517538\n",
      "EPOCH: 26 LOSS: 34.37727699923115\n",
      "EPOCH: 27 LOSS: 33.75653315566698\n",
      "EPOCH: 28 LOSS: 33.14593592840209\n",
      "EPOCH: 29 LOSS: 32.545283988777804\n",
      "EPOCH: 30 LOSS: 31.954389667130716\n",
      "EPOCH: 31 LOSS: 31.373078014547204\n",
      "EPOCH: 32 LOSS: 30.801185893609578\n",
      "EPOCH: 33 LOSS: 30.23856109720871\n",
      "EPOCH: 34 LOSS: 29.685061494744126\n",
      "EPOCH: 35 LOSS: 29.14055420526259\n",
      "EPOCH: 36 LOSS: 28.60491479730109\n",
      "EPOCH: 37 LOSS: 28.078026515401177\n",
      "EPOCH: 38 LOSS: 27.559779533448104\n",
      "EPOCH: 39 LOSS: 27.05007023516067\n",
      "EPOCH: 40 LOSS: 26.548800522212733\n",
      "EPOCH: 41 LOSS: 26.055877150605085\n",
      "EPOCH: 42 LOSS: 25.571211096022143\n",
      "EPOCH: 43 LOSS: 25.09471694899956\n",
      "EPOCH: 44 LOSS: 24.626312340792683\n",
      "EPOCH: 45 LOSS: 24.165917400869166\n",
      "EPOCH: 46 LOSS: 23.713454246948267\n",
      "EPOCH: 47 LOSS: 23.26884650847372\n",
      "EPOCH: 48 LOSS: 22.832018884333728\n",
      "EPOCH: 49 LOSS: 22.402896735532792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# 超参数\n",
    "setting = {\n",
    "    \"window\": 2,  # 窗口尺寸（至左边或右边，即左边2个和右边2个）\n",
    "    \"n\": 8,  # 词向量维度\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.01\n",
    "}\n",
    "\n",
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = setting['n']\n",
    "        self.window = setting['window']\n",
    "        self.epochs = setting['epochs']\n",
    "        self.learning_rate = setting['learning_rate']\n",
    "\n",
    "    def generate_train_data(self, setting, corpus_file):\n",
    "        \"\"\"建立训练用的词向量\"\"\"\n",
    "        # 计算非重复词语的频数\n",
    "        word_counts = defaultdict(int)\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                row = [word.lower() for word in line.strip().split()]\n",
    "                for word in row:\n",
    "                    word_counts[word] += 1\n",
    "\n",
    "        self.count_word = len(word_counts.keys())  # 非重复词语数量\n",
    "        self.words_list = list(word_counts.keys())  # 非重语料列表\n",
    "        self.word_idx = dict((word, i) for i, word in enumerate(self.words_list))  # 建立word_idx 字典，加快访问速度\n",
    "        self.idx_word = dict((i, word) for i, word in enumerate(self.words_list))  # 建立idx_word 字典，加快访问速度\n",
    "\n",
    "        training_data = []\n",
    "\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                setence = [word.lower() for word in line.strip().split()]  # 遍历每行（每句话）\n",
    "                len_setence = len(setence)\n",
    "                for i, word in enumerate(setence):  # 遍历每个词（目标词）\n",
    "                    w_target = self.word2onehot(setence[i])  # 把词语转化成向量\n",
    "                    w_context = []  # 上下文\n",
    "\n",
    "                    # 获取上下文向量\n",
    "                    for j in range(i - self.window, i + self.window + 1):  # 遍历窗口内的上下文\n",
    "                        if j != i and j >= 0 and j < len_setence:\n",
    "                            w_context.append(self.word2onehot(setence[j]))  # 把上下文的词语转化为one-hot向量\n",
    "\n",
    "                    # 把向量保存到训练集列表中\n",
    "                    training_data.append([w_target, w_context])  # 每个词语的向量和该词语的上下文向量加入到训练集中\n",
    "        return np.array(training_data)\n",
    "\n",
    "    def word2onehot(self, word):\n",
    "        \"\"\"建立one-hot词向量\"\"\"\n",
    "        word_vec = np.zeros(self.count_word)  # 建立长度为词语数量的全0向量\n",
    "\n",
    "        word_index = self.word_idx[word]  # 获取这个词语独一无二的编号\n",
    "        word_vec[word_index] = 1  # 将全0向量中编号处的 0 改成 1 ，该词语的one-hot向量\n",
    "        return word_vec\n",
    "\n",
    "    def train(self, training_data):\n",
    "        \"\"\"训练\"\"\"\n",
    "        # 初始化权重\n",
    "        self.w1 = np.random.uniform(-1, 1, (self.count_word, self.n))  # shape = （count_word ， n）\n",
    "        self.w2 = np.random.uniform(-1, 1, (self.n, self.count_word))  # shape = （n ， count_vec）\n",
    "\n",
    "        # 遍历每个epoch\n",
    "        for i in range(self.epochs):\n",
    "            self.loss = 0  # 初始化loss\n",
    "            for w_t, w_c in training_data:  # 遍历每个训练集数据\n",
    "                # 前向传播\n",
    "                y_predict, h, u = self.forward(w_t)\n",
    "                # 计算误差\n",
    "                EI = np.sum([np.subtract(y_predict, word) for word in w_c], axis=0)\n",
    "                # 反向传播，并更新参数\n",
    "                self.backprop(EI, h, w_t)\n",
    "                # 计算误差\n",
    "                self.loss += -np.sum([u[word.tolist().index(1)] for word in w_c]) + len(w_c) * np.log(\n",
    "                    np.sum(np.exp(u)))\n",
    "            print('EPOCH:', i, 'LOSS:', self.loss)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        h = np.dot(self.w1.T, x)  # 隐藏层的输出(w1.T的形状是（n,count_word）,x的形状是（count_word,）所以结果的形状为（n,）)\n",
    "        u = np.dot(self.w2.T, h)  # 输出层的输出(w2.T的形状是（count_word,n）,h的形状是（n,),所以结果的形状是(count_word,)\n",
    "\n",
    "        y_c = self.softmax(u)  # 激活层输出\n",
    "        return y_c, h, u\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"softmax\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))  # x的形状为(count_word,)\n",
    "        return e_x / np.sum(e_x, axis=0)\n",
    "\n",
    "    def backprop(self, e, h, x):\n",
    "        dl_dw2 = np.outer(h, e)  # 损失函数对w2偏导\n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))  # 损失函数对w1偏导\n",
    "\n",
    "        # update weight\n",
    "        self.w1 = self.w1 - (self.learning_rate * dl_dw1)  # 更新w\n",
    "        self.w2 = self.w2 - (self.learning_rate * dl_dw2)  # 更新w\n",
    "\n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_idx[word]\n",
    "        v_w = self.w1[w_index]\n",
    "        return v_w\n",
    "\n",
    "# 实例化模型\n",
    "w2v = word2vec()\n",
    "training_data = w2v.generate_train_data(setting, r'C:\\Users\\zhangxiaomi\\Desktop\\chengyu.txt')#中文小语料库\n",
    "print(training_data.shape)\n",
    "\n",
    "w2v.train(training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
