{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233cb0f0-d821-40a1-8c2c-304a15d1a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fc2bf3-95d0-4479-b63f-a83f8d697f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_CUDA: True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print('USE_CUDA: %s' % USE_CUDA)\n",
    "SEGMENTATION = True    # 是否分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865cc9af-5571-49e0-a98b-2d368eafe65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.!?，。？]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9491f03-ce02-4b33-bd37-f8b1c9a4d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK_token = 2\n",
    "\n",
    "class Lang(object):\n",
    "    \"\"\"\n",
    "    词表Vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", '2': 'UNK'}\n",
    "        self.n_words = 3 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        if self.name == 'cn':\n",
    "            words = list(jieba.cut(sentence)) if SEGMENTATION else sentence    \n",
    "            for word in words:\n",
    "                self.index_word(word)\n",
    "        else:\n",
    "            words = sentence.split(' ')\n",
    "            for word in words:\n",
    "                self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4081ee41-9fce-45db-8b90-8e0f33ec1569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04305de-f262-4d44-b491-1f4dc6349a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79122520-a762-4e9e-8f74-5f78954d242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 90000 sentence pairs\n",
      "Trimmed to 68898 sentence pairs\n",
      "Indexing words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.597 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我会出席会议的。', 'i will be at the meeting .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('cn', 'eng', False)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dce8645-4ce5-4c50-a96f-6a6d029c2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test_dataset(size=100):\n",
    "\n",
    "    with open('cn-eng-test.txt', 'w+') as f:\n",
    "        f.write('\\n'.join(['\\t'.join(pair) for pair in random.sample(pairs, k=size)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d6f4ed-6f2f-4792-a8ce-59494ca5e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    \"\"\"\n",
    "    根据词表，将句子转化成索引列表。\n",
    "\n",
    "    :reutrn list，e.g. [1, 2, 3, 4]\n",
    "    \"\"\"\n",
    "    if lang.name == 'cn':\n",
    "        words = list(jieba.cut(sentence)) if SEGMENTATION else sentence\n",
    "        return [lang.word2index[word] if word in lang.word2index else UNK_token for word in words ]\n",
    "    else:\n",
    "        words = sentence.split(' ')\n",
    "        return [lang.word2index[word] if word in lang.word2index else UNK_token for word in words]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    \"\"\"\n",
    "    将句子转换成Tensor.\n",
    "    \n",
    "    :return Tensor, shape(n, 1)\n",
    "    \"\"\"\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = torch.LongTensor(indexes).view(-1, 1)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    \"\"\"\n",
    "    将平行语料对转化成Tensors.\n",
    "    \n",
    "    :return (input_tensor, output_tensor)\n",
    "    \"\"\"\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b15da9-3e26-4895-b391-34c3631d3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair: ['你早點回來，行嗎？', 'you come back soon ok ?']\n",
      "input_tensor shape: torch.Size([8, 1]), output_tensor shap: torch.Size([7, 1])\n",
      "input_tensor: tensor([[  13],\n",
      "        [1238],\n",
      "        [  29],\n",
      "        [  21],\n",
      "        [2911],\n",
      "        [ 538],\n",
      "        [  17],\n",
      "        [   1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pair = random.choice(pairs)\n",
    "print('pair: %s' % pair)\n",
    "\n",
    "input_tensor, target_tensor = variables_from_pair(pair)\n",
    "print('input_tensor shape: %s, output_tensor shap: %s' % (input_tensor.shape, target_tensor.shape))\n",
    "print('input_tensor: %s' % input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb36b5b7-eeca-4814-bf2d-2a55cbd4a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    \"\"\"GRU 编码器\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, bidirectional=False):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param input_size, 输入词表大\n",
    "        :param hidden_size, Embedding维度大小，RNN hidden大小\n",
    "        :param n_layers, RNN层数\n",
    "        \"\"\"\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # 用GRU替换RNN\n",
    "        # self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param word_inputs, 输入序列 shape(n, 1)\n",
    "        :param hidden, 隐层 shape(seq_len*n_layers, batch_size, hidden_size)\n",
    "        :return output(seq_len, batch, num_directions*hidden_size),\n",
    "                hidden(num_layers*num_directions, hidden_size)\n",
    "        \"\"\"\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        hidden = torch.zeros(self.n_layers*num_directions, 1, self.hidden_size)\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a14d784-037a-4b90-8f8a-936752887cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # BahdanauAttention\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param decoder_hidden: shape(num_layers*num_directions, batch, hidden_size)\n",
    "        :param encoder_outputs: shape(seq_len, batch, num_directions*hidden_size)\n",
    "        :return attention_weighted_encoder_output shape(num_layers, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        query = decoder_hidden[-1]  # 只取decoder最后一层的隐藏状态\n",
    "        query = query.unsqueeze(1)  # 增加维度以便与encoder_outputs相加\n",
    "        energy = torch.tanh(self.W(query) + self.U(encoder_outputs))\n",
    "        attention = F.softmax(self.v(energy), dim=0)\n",
    "        context = attention * encoder_outputs\n",
    "        attn_output = torch.sum(context, dim=0)\n",
    "        return attn_output.view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb52b30-aabf-4a4d-8ef7-5460e12d1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    \"\"\"注意力机制解码器\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.attention = Attention(hidden_size)\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\n",
    "        # 使用GRU替换RNN\n",
    "        # self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size*2, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time        \n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        rnn_output, hidden = self.rnn(word_embedded, last_hidden)\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "\n",
    "        # attention weighted encoder output\n",
    "        attn_weighted_encoder_output = self.attention(hidden, encoder_outputs)\n",
    "        attn_weighted_encoder_output = attn_weighted_encoder_output.squeeze(0)\n",
    "        \n",
    "        concat_output = torch.cat([rnn_output, attn_weighted_encoder_output], dim=1)\n",
    "        output = F.log_softmax(self.out(concat_output))\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e057a582-f182-4d7f-b8dc-f962d25098b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, \n",
    "          encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, \n",
    "          criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = torch.LongTensor([[SOS_token]])\n",
    "    # Use last hidden state from encoder to start decoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di] # Next target is next input\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use network's own prediction as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            \n",
    "            # Get most likely word index (highest value) from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            \n",
    "            decoder_input = torch.LongTensor([[ni]]) # Chosen word is next input\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9041dcb-c3b2-4794-a2bb-bcb01579c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e52d9d-08cc-4909-82fd-d2385eeeab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/tmp/ipykernel_2839/3848772925.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(concat_output))\n",
      "/tmp/ipykernel_2839/3644649550.py:57: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
      "/tmp/ipykernel_2839/3644649550.py:58: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/150000, 0m 15s (- 37m 40s), 4.9200\n",
      "Epoch 2000/150000, 0m 30s (- 37m 54s), 4.5888\n",
      "Epoch 3000/150000, 0m 46s (- 38m 12s), 4.4243\n",
      "Epoch 4000/150000, 1m 3s (- 38m 52s), 4.2580\n",
      "Epoch 5000/150000, 1m 28s (- 42m 53s), 4.2194\n",
      "Epoch 6000/150000, 1m 52s (- 45m 4s), 4.1849\n",
      "Epoch 7000/150000, 2m 17s (- 46m 46s), 4.0790\n",
      "Epoch 8000/150000, 2m 42s (- 48m 5s), 4.0197\n",
      "Epoch 9000/150000, 3m 6s (- 48m 42s), 3.9448\n",
      "Epoch 10000/150000, 3m 30s (- 49m 4s), 3.9106\n",
      "Epoch 11000/150000, 3m 54s (- 49m 21s), 3.8593\n",
      "Epoch 12000/150000, 4m 18s (- 49m 32s), 3.8151\n",
      "Epoch 13000/150000, 4m 42s (- 49m 41s), 3.7843\n",
      "Epoch 14000/150000, 5m 6s (- 49m 41s), 3.7578\n",
      "Epoch 15000/150000, 5m 31s (- 49m 39s), 3.6588\n",
      "Epoch 16000/150000, 5m 55s (- 49m 33s), 3.6501\n",
      "Epoch 17000/150000, 6m 19s (- 49m 29s), 3.6408\n",
      "Epoch 18000/150000, 6m 43s (- 49m 22s), 3.5317\n",
      "Epoch 19000/150000, 7m 7s (- 49m 9s), 3.5419\n",
      "Epoch 20000/150000, 7m 31s (- 48m 55s), 3.4147\n",
      "Epoch 21000/150000, 7m 55s (- 48m 42s), 3.4810\n",
      "Epoch 22000/150000, 8m 20s (- 48m 29s), 3.4173\n",
      "Epoch 23000/150000, 8m 44s (- 48m 15s), 3.4028\n",
      "Epoch 24000/150000, 9m 8s (- 48m 0s), 3.3112\n",
      "Epoch 25000/150000, 9m 33s (- 47m 46s), 3.4476\n",
      "Epoch 26000/150000, 9m 57s (- 47m 29s), 3.4125\n",
      "Epoch 27000/150000, 10m 21s (- 47m 13s), 3.2846\n",
      "Epoch 28000/150000, 10m 45s (- 46m 53s), 3.2778\n",
      "Epoch 29000/150000, 11m 10s (- 46m 36s), 3.2478\n",
      "Epoch 30000/150000, 11m 34s (- 46m 17s), 3.2577\n",
      "Epoch 31000/150000, 11m 58s (- 45m 58s), 3.1438\n",
      "Epoch 32000/150000, 12m 22s (- 45m 39s), 3.1002\n",
      "Epoch 33000/150000, 12m 47s (- 45m 19s), 3.1109\n",
      "Epoch 34000/150000, 13m 11s (- 45m 0s), 3.0875\n",
      "Epoch 35000/150000, 13m 35s (- 44m 40s), 3.0371\n",
      "Epoch 36000/150000, 14m 0s (- 44m 20s), 3.0285\n",
      "Epoch 37000/150000, 14m 24s (- 44m 1s), 3.0385\n",
      "Epoch 38000/150000, 14m 49s (- 43m 41s), 3.0082\n",
      "Epoch 39000/150000, 15m 13s (- 43m 19s), 2.9567\n",
      "Epoch 40000/150000, 15m 37s (- 42m 59s), 3.0289\n",
      "Epoch 41000/150000, 16m 2s (- 42m 38s), 2.9483\n",
      "Epoch 42000/150000, 16m 26s (- 42m 17s), 2.9725\n",
      "Epoch 43000/150000, 16m 51s (- 41m 56s), 2.9013\n",
      "Epoch 44000/150000, 17m 15s (- 41m 35s), 2.9165\n",
      "Epoch 45000/150000, 17m 40s (- 41m 13s), 2.8878\n",
      "Epoch 46000/150000, 18m 4s (- 40m 51s), 2.8982\n",
      "Epoch 47000/150000, 18m 29s (- 40m 30s), 2.8259\n",
      "Epoch 48000/150000, 18m 53s (- 40m 8s), 2.8440\n",
      "Epoch 49000/150000, 19m 18s (- 39m 47s), 2.7946\n",
      "Epoch 50000/150000, 19m 42s (- 39m 24s), 2.7408\n",
      "Epoch 51000/150000, 20m 6s (- 39m 2s), 2.7406\n",
      "Epoch 52000/150000, 20m 30s (- 38m 39s), 2.7069\n",
      "Epoch 53000/150000, 20m 55s (- 38m 17s), 2.7995\n",
      "Epoch 54000/150000, 21m 19s (- 37m 54s), 2.7371\n",
      "Epoch 55000/150000, 21m 43s (- 37m 31s), 2.7003\n",
      "Epoch 56000/150000, 22m 8s (- 37m 9s), 2.6571\n",
      "Epoch 57000/150000, 22m 32s (- 36m 46s), 2.7001\n",
      "Epoch 58000/150000, 22m 56s (- 36m 23s), 2.6273\n",
      "Epoch 59000/150000, 23m 21s (- 36m 1s), 2.6206\n",
      "Epoch 60000/150000, 23m 45s (- 35m 38s), 2.6788\n",
      "Epoch 61000/150000, 24m 10s (- 35m 16s), 2.6280\n",
      "Epoch 62000/150000, 24m 35s (- 34m 54s), 2.6593\n",
      "Epoch 63000/150000, 25m 0s (- 34m 31s), 2.6282\n",
      "Epoch 64000/150000, 25m 24s (- 34m 8s), 2.6109\n",
      "Epoch 65000/150000, 25m 49s (- 33m 45s), 2.5094\n",
      "Epoch 66000/150000, 26m 13s (- 33m 22s), 2.5536\n",
      "Epoch 67000/150000, 26m 38s (- 32m 59s), 2.4610\n",
      "Epoch 68000/150000, 27m 2s (- 32m 36s), 2.5476\n",
      "Epoch 69000/150000, 27m 27s (- 32m 13s), 2.5118\n",
      "Epoch 70000/150000, 27m 51s (- 31m 50s), 2.5026\n",
      "Epoch 71000/150000, 28m 16s (- 31m 27s), 2.5613\n",
      "Epoch 72000/150000, 28m 41s (- 31m 4s), 2.4832\n",
      "Epoch 73000/150000, 29m 7s (- 30m 43s), 2.5498\n",
      "Epoch 74000/150000, 29m 33s (- 30m 21s), 2.4021\n",
      "Epoch 75000/150000, 29m 58s (- 29m 58s), 2.3166\n",
      "Epoch 76000/150000, 30m 22s (- 29m 34s), 2.3770\n",
      "Epoch 77000/150000, 30m 47s (- 29m 11s), 2.4595\n",
      "Epoch 78000/150000, 31m 12s (- 28m 48s), 2.4104\n",
      "Epoch 79000/150000, 31m 36s (- 28m 24s), 2.3267\n",
      "Epoch 80000/150000, 32m 1s (- 28m 0s), 2.4047\n",
      "Epoch 81000/150000, 32m 25s (- 27m 37s), 2.3947\n",
      "Epoch 82000/150000, 32m 50s (- 27m 14s), 2.3287\n",
      "Epoch 83000/150000, 33m 15s (- 26m 50s), 2.3988\n",
      "Epoch 84000/150000, 33m 40s (- 26m 27s), 2.4495\n",
      "Epoch 85000/150000, 34m 5s (- 26m 4s), 2.3447\n",
      "Epoch 86000/150000, 34m 32s (- 25m 41s), 2.2964\n",
      "Epoch 87000/150000, 34m 58s (- 25m 19s), 2.3553\n",
      "Epoch 88000/150000, 35m 23s (- 24m 55s), 2.3276\n",
      "Epoch 89000/150000, 35m 48s (- 24m 32s), 2.2309\n",
      "Epoch 90000/150000, 36m 12s (- 24m 8s), 2.3251\n",
      "Epoch 91000/150000, 36m 37s (- 23m 44s), 2.2863\n",
      "Epoch 92000/150000, 37m 4s (- 23m 22s), 2.2071\n",
      "Epoch 93000/150000, 37m 28s (- 22m 58s), 2.3268\n",
      "Epoch 94000/150000, 37m 53s (- 22m 34s), 2.2231\n",
      "Epoch 95000/150000, 38m 18s (- 22m 10s), 2.2002\n",
      "Epoch 96000/150000, 38m 42s (- 21m 46s), 2.1716\n",
      "Epoch 97000/150000, 39m 6s (- 21m 22s), 2.2189\n",
      "Epoch 98000/150000, 39m 31s (- 20m 58s), 2.1910\n",
      "Epoch 99000/150000, 39m 56s (- 20m 34s), 2.1843\n",
      "Epoch 100000/150000, 40m 21s (- 20m 10s), 2.1421\n",
      "Epoch 101000/150000, 40m 46s (- 19m 46s), 2.0781\n",
      "Epoch 102000/150000, 41m 11s (- 19m 22s), 2.2743\n",
      "Epoch 103000/150000, 41m 36s (- 18m 59s), 2.2007\n",
      "Epoch 104000/150000, 42m 1s (- 18m 35s), 2.0938\n",
      "Epoch 105000/150000, 42m 26s (- 18m 11s), 2.1605\n",
      "Epoch 106000/150000, 42m 50s (- 17m 47s), 2.0994\n",
      "Epoch 107000/150000, 43m 15s (- 17m 22s), 2.1146\n",
      "Epoch 108000/150000, 43m 40s (- 16m 59s), 2.1012\n",
      "Epoch 109000/150000, 44m 5s (- 16m 34s), 2.1549\n",
      "Epoch 110000/150000, 44m 30s (- 16m 10s), 2.1393\n",
      "Epoch 111000/150000, 44m 55s (- 15m 46s), 2.0695\n",
      "Epoch 112000/150000, 45m 20s (- 15m 23s), 2.1931\n",
      "Epoch 113000/150000, 45m 45s (- 14m 59s), 2.1223\n",
      "Epoch 114000/150000, 46m 10s (- 14m 34s), 1.9751\n",
      "Epoch 115000/150000, 46m 34s (- 14m 10s), 2.0302\n",
      "Epoch 116000/150000, 46m 59s (- 13m 46s), 2.0139\n",
      "Epoch 117000/150000, 47m 24s (- 13m 22s), 2.0233\n",
      "Epoch 118000/150000, 47m 49s (- 12m 58s), 1.9484\n",
      "Epoch 119000/150000, 48m 14s (- 12m 33s), 2.0487\n",
      "Epoch 120000/150000, 48m 39s (- 12m 9s), 2.0068\n",
      "Epoch 121000/150000, 49m 4s (- 11m 45s), 2.0412\n",
      "Epoch 122000/150000, 49m 30s (- 11m 21s), 2.0201\n",
      "Epoch 123000/150000, 49m 56s (- 10m 57s), 1.9378\n",
      "Epoch 124000/150000, 50m 21s (- 10m 33s), 2.0745\n",
      "Epoch 125000/150000, 50m 46s (- 10m 9s), 1.9025\n",
      "Epoch 126000/150000, 51m 10s (- 9m 44s), 1.9336\n",
      "Epoch 127000/150000, 51m 36s (- 9m 20s), 1.9461\n",
      "Epoch 128000/150000, 52m 0s (- 8m 56s), 1.9396\n",
      "Epoch 129000/150000, 52m 25s (- 8m 32s), 1.9786\n",
      "Epoch 130000/150000, 52m 50s (- 8m 7s), 1.9345\n",
      "Epoch 131000/150000, 53m 15s (- 7m 43s), 1.9355\n",
      "Epoch 132000/150000, 53m 40s (- 7m 19s), 2.0010\n",
      "Epoch 133000/150000, 54m 5s (- 6m 54s), 1.8681\n",
      "Epoch 134000/150000, 54m 30s (- 6m 30s), 1.9654\n",
      "Epoch 135000/150000, 54m 54s (- 6m 6s), 1.8658\n",
      "Epoch 136000/150000, 55m 19s (- 5m 41s), 1.8404\n",
      "Epoch 137000/150000, 55m 44s (- 5m 17s), 1.9087\n",
      "Epoch 138000/150000, 56m 9s (- 4m 53s), 1.8615\n",
      "Epoch 139000/150000, 56m 34s (- 4m 28s), 1.9718\n",
      "Epoch 140000/150000, 56m 59s (- 4m 4s), 1.9267\n",
      "Epoch 141000/150000, 57m 24s (- 3m 39s), 1.8730\n",
      "Epoch 142000/150000, 57m 49s (- 3m 15s), 1.7911\n",
      "Epoch 143000/150000, 58m 11s (- 2m 50s), 1.8797\n",
      "Epoch 144000/150000, 58m 31s (- 2m 26s), 1.8325\n",
      "Epoch 145000/150000, 58m 49s (- 2m 1s), 1.7762\n",
      "Epoch 146000/150000, 59m 8s (- 1m 37s), 1.8191\n",
      "Epoch 147000/150000, 59m 27s (- 1m 12s), 1.8193\n",
      "Epoch 148000/150000, 59m 45s (- 0m 48s), 1.7993\n",
      "Epoch 149000/150000, 60m 4s (- 0m 24s), 1.7706\n",
      "Epoch 150000/150000, 60m 20s (- 0m 0s), 1.8424\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 500\n",
    "n_layers = 1\n",
    "dropout_p = 0.05\n",
    "n_epochs = 150000\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderGRU(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = DecoderGRU(hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Configuring training\n",
    "plot_every = 200\n",
    "print_every = 1000\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every\n",
    "\n",
    "# Begin!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    training_pair = variables_from_pair(random.choice(pairs))\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, \n",
    "                 encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = 'Epoch %d/%d, %s, %.4f' % (epoch, n_epochs, time_since(start, epoch / n_epochs), \n",
    "                                                   print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55dad5a4-ae25-41c0-ad69-09b4dfcdceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6326a085-969f-48ff-bf75-407c4501310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = torch.LongTensor([[SOS_token]]) # SOS\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni.item()])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = torch.LongTensor([[ni]])\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3024e586-db16-453b-801e-12feaf930e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e45951d4-b509-4b90-9663-22b334a805f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 那个日文词汇没有对应的英文。\n",
      "= that japanese word has no equivalent in english .\n",
      "< that japanese japanese has no no in without english .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2839/3848772925.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(concat_output))\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0c5f418-1426-48b7-85fc-a387f563e7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2839/3848772925.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(concat_output))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'life is fun . <EOS>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(evaluate('人生是有趣的。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a2bdac9-5e5e-49e6-9ab4-0322b605b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "\n",
    "# 读取测试数据集\n",
    "with open('cn-eng-test.txt') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    \n",
    "    test_pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "\n",
    "\n",
    "test_pairs_dict = collections.defaultdict(lambda : [])\n",
    "\n",
    "for pair in test_pairs:\n",
    "    test_pairs_dict[pair[0]].append(pair[1].split(' '))\n",
    "\n",
    "\n",
    "def evaluate_bleu_score():\n",
    "    candicates = []\n",
    "    references = []\n",
    "\n",
    "    for i, pair in enumerate(test_pairs_dict.items(), start=1):\n",
    "        candicate = evaluate(pair[0])\n",
    "        if candicate[-1] == '<EOS>':\n",
    "            candicate.pop(-1)\n",
    "        candicates.append(candicate)\n",
    "        references.append(pair[1])\n",
    "    \n",
    "    score = bleu_score(candicates, references)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ceb9ede-06be-495e-87f6-300e610b6504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2839/3848772925.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(concat_output))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset bleu score: 0.3768230140101369\n"
     ]
    }
   ],
   "source": [
    "print('test dataset bleu score: %s' % evaluate_bleu_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "409fa2af-05fd-4c9e-8ba8-7dd7ac230234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(filepath):\n",
    "    \"\"\"预测\"\"\"\n",
    "    with open('test.txt') as f:\n",
    "        sentences =  [line.strip() for line in f.readlines()]\n",
    "\n",
    "    output_sentences = []\n",
    "    for sentence in sentences:\n",
    "        output_sentence = ' '.join(evaluate(sentence))\n",
    "        output_sentences.append(output_sentence.strip('<EOS>'))\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('\\n'.join(output_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8db6af12-1d84-454e-be62-47fc58069528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2839/3848772925.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(concat_output))\n"
     ]
    }
   ],
   "source": [
    "predict('result-attention.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd9011-7c3d-43f7-bc3b-4a6cc4c7c5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9e89d-3f36-48c2-b340-d4d585e3f442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4801d-0d5c-4b79-b13f-5afd1f022ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa31519-5c58-441c-aa24-011db7f33991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafdddd0-0a46-41d0-bb1a-2b23543a26a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
